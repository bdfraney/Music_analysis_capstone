{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "from beat_features import beat_feat_parser, static_tempo # Importing from .py file to keep things more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnb = r\"D:\\Music\\DnB\\beatjunkierato+dnbpanacea1hitfreedl.mp3\"\n",
    "beat = beat_feat_parser(dnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beat features\n",
    "\n",
    "The beat features is based on the [\"Advanced Usage\"](https://librosa.github.io/librosa/tutorial.html#advanced-usage) section in their tutorial.  This extracts features by \"integrating harmonic-percussive separation, multiple spectral features, and beat-synchronous feature aggregation.\" I'll give a (mostly) high level overview of what these mean and why I chose to utilize them.  I'll also explain the behind-the-scenes action of the function because the actual code isn't that easy to understand without some prior knowledge.\n",
    "\n",
    "In general, using raw features from an audio time series is not the best route to go.  There are many reasons for this (different scales, units, etc) but one example is a problem I encountered before I got this working.  The resultant output of the function gave a different NumPy array for each file.  One axis was constant: it was the index for the feature \"names.\"  The pitch chroma thats computed has 12 components that correspond to the instensity of each pitch (by [pitch](https://en.wikipedia.org/wiki/Pitch_(music)) I mean the familiar C, D, E, F, G, A, B scale).  The other axis was the value of the coefficient at a frame, which is based on the length of the audio file.  Besides the fact that dealing with a 12x16000 tensor isn't particularly fun, the size difference also is an issue for comparing audio tracks.  \n",
    "\n",
    "This can be handled by aggregating the features in a multitude of ways.  You can do this statistically with the mean, median, standard deviation, etc, or change how you extract the features, or some other clever way that gets you want you need.  Additionally, these aggregations actually can give you more info about the audio file because there could be meaning in the statistical features.  I chose to use the arithmetic mean because it was easy to implement and it would still give a meaningful representation of each song.\n",
    "\n",
    "Now to look at what was actually aggregated and how I got them.\n",
    "\n",
    "#### Harmonic-Percussive Source Separation (HPSS)\n",
    "\n",
    "Without going too much into the math behind this, HPSS takes the Fourier transform of short windows (on the order of ms) that slightly overlap which then allows you to separate the harmonic and percussive elements of the song.  This allows easier extraction of harmonic features like pitch and percussive features like rhythm/tempo.  \n",
    "\n",
    "#### Mel-frequency cepstral coefficients (MFCCs)\n",
    "\n",
    "See [LibROSA EDA](./notebooks/Notebooks/EDA_LibROSA.ipynb)\n",
    "\n",
    "#### MFCC Deltas\n",
    "\n",
    "The smooth first order differences between the MFCC columns.  (Same as in time series analysis).\n",
    "\n",
    "#### Beat MFCC Delta\n",
    "\n",
    "The MFCCs and are aggregated by matching them to a time index (beat frames in this case).\n",
    "\n",
    "#### Chromagram\n",
    "\n",
    "Same thing as the [pitch chroma]() from the LibROSA EDA, although this one is computed with a slightly different method.  Explaining the differences would involve too much math for this but you can read more if you're curious: \n",
    "- [Short-time Fourier Transform](https://en.wikipedia.org/wiki/Short-time_Fourier_transform)  \n",
    "- [Constant-Q transform](https://en.wikipedia.org/wiki/Constant-Q_transform)\n",
    "\n",
    "#### Beat Chroma\n",
    "\n",
    "Similar to the beat MFCC delta but with the chromagram.  This is also aggregated with the median instead of the mean.\n",
    "\n",
    "---\n",
    "\n",
    "The output is a single feature vector for the audio file to be used in the neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('subset_beat.npy', beat_features, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.loadtxt('./data/subset_beat.npy', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting the static tempo for each track**\n",
    "\n",
    "I looped through each genre individually so I could set the proper initial bpm estimate.  The accuracy of the algorithm is highly dependent on the initial estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempos = np.empty((1,))\n",
    "# for files in glob.glob(\"D:\\MusicSubset\\DnB\\*.mp3\"):\n",
    "#     tempos = np.vstack((tempos,static_tempo(files, bpm_estimate=140)))\n",
    "# tempos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for files in glob.glob(\"D:\\MusicSubset\\Dubstep\\*.mp3\"):\n",
    "#     tempos = np.vstack((tempos,static_tempo(files, bpm_estimate=150)))\n",
    "# tempos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for files in glob.glob(\"D:\\MusicSubset\\Garage\\*.mp3\"):\n",
    "#     tempos = np.vstack((tempos,static_tempo(files, bpm_estimate=120)))\n",
    "# tempos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for files in glob.glob(\"D:\\MusicSubset\\House\\*.mp3\"):\n",
    "#     tempos = np.vstack((tempos,static_tempo(files, bpm_estimate=120)))\n",
    "# tempos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for files in glob.glob(\"D:\\MusicSubset\\Indie\\*.mp3\"):\n",
    "#     tempos = np.vstack((tempos,static_tempo(files, bpm_estimate=120)))\n",
    "# tempos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for files in glob.glob(\"D:\\MusicSubset\\Techno\\*.mp3\"):\n",
    "#     tempos = np.vstack((tempos,static_tempo(files, bpm_estimate=150)))\n",
    "# tempos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"subset_tempos1.npy\", tempos, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to speed up the process with pooling.  It wasn't very successful, which I will explain in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\Music\\DnB\\*.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feats = np.empty((92,))\n",
    "# feats = np.dstack(pool.map(beat_feat_parser, [file for file in glob.glob(path)]))\n",
    "# pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = pd.DataFrame(beat_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.to_csv('feats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('../data/track_df.csv').shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}